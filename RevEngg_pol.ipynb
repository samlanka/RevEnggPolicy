{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of reverse-policy Q-learning using OpenAI FrozenLake as Gridworld template ##\n",
    "\n",
    "### Sameera Lanka ###\n",
    "### North Carolina State University ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from gym.envs.registration import register\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "#Create a new register enntry with FrozenLake environment to create\n",
    "#with is_slippery=0 for deterministic actions \n",
    "register(\n",
    "    id='FrozenLakeNew-v0',\n",
    "    entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "    kwargs={'map_name' : '8x8', 'is_slippery': False},\n",
    "    max_episode_steps=200,\n",
    "    reward_threshold=0.99, # optimum = 1\n",
    ")\n",
    "\n",
    "env = gym.make('FrozenLakeNew-v0')\n",
    "reward = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Qlearn:\n",
    "    \"\"\"Define Qlearning agent\"\"\"\n",
    "    def __init__(self, env):\n",
    "        self.action_space = env.action_space\n",
    "        self.action_dict = dict([(0, 'Left'),(1,'Down'),(2,'Right'), (3,'Up')])\n",
    "        self.state_space = env.observation_space\n",
    "        self.q_table = np.zeros([self.state_space.n, self.action_space.n])\n",
    "        \n",
    "        self.discount = 0.9 #discount factor\n",
    "        self.step = 0.01 #step size\n",
    "        self.eps = 0.4 #initial epsilon\n",
    "        \n",
    "    def policy_eval(self):\n",
    "        state_t = env.reset()\n",
    "    \n",
    "        for num in range(1000): #num of episodes = 1000\n",
    "       \n",
    "            state_t = env.reset()\n",
    "            action = self.policy_iter(state_t)\n",
    "            done = False\n",
    "            episode_reward = 0\n",
    "                        \n",
    "            while not done:\n",
    "                \n",
    "                state_t1, reward_t1, done, info = env.step(action)\n",
    "                episode_reward += reward_t1\n",
    "                \n",
    "                if done:\n",
    "                    target = reward_t1\n",
    "                    self.q_table[state_t, action] += self.step*(target - self.q_table[state_t, action])\n",
    "                    \n",
    "                    break\n",
    "                \n",
    "                else:\n",
    "                    target = reward_t1 + self.discount*(np.amax(self.q_table[state_t1,:]))\n",
    "                    self.q_table[state_t, action] += self.step*(target - self.q_table[state_t, action])\n",
    "                    episode_reward += reward_t1\n",
    "                    \n",
    "                    state_t = state_t1\n",
    "                    action = self.policy_iter(state_t)\n",
    "                    \n",
    "            self.eps *= 0.99 #exponential decay of epsilon per episode\n",
    "            reward.append(episode_reward)\n",
    "\n",
    "        \n",
    "    def policy_iter(self, state):\n",
    " \n",
    "        if np.random.random() > self.eps:\n",
    "            #Random tie-breaking between max values\n",
    "            q_max = np.amax(self.q_table[state,:])\n",
    "            act_max = np.nonzero((self.q_table[state,:] == q_max))[0]\n",
    "            action = np.random.choice(act_max)\n",
    "           \n",
    "        else:\n",
    "            #Select random action\n",
    "            action = self.action_space.sample()\n",
    "        \n",
    "        return action\n",
    "    \n",
    "    def final_policy(self):\n",
    "        \n",
    "        state_t = env.reset()\n",
    "        env.render()\n",
    "        done = False\n",
    "        print \"The final forward policy is\"\n",
    "        while not done:\n",
    "            action = self.policy_iter(state_t)\n",
    "            state_t1, _, done, _ = env.step(action)\n",
    "            env.render()\n",
    "            print \"{:d} -> {:d}, {}\".format(state_t, state_t1, self.action_dict[action])\n",
    "            state_t = state_t1\n",
    "            action = self.policy_iter(state_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    \n",
    "    tabq = Qlearn(env)\n",
    "    tabq.policy_eval()\n",
    "    tabq.final_policy()\n",
    "   \n",
    "    plt.xlabel('Episode number')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.plot(reward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if __name__==\"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
